# @package _global_

# to execute this experiment run:
# python train.py +experiment=train_GATsSPG

defaults:
    - override /trainer: null  # override trainer to null so it's not loaded from main config defaults...
    - override /model: null
    - override /datamodule: null
    - override /callbacks: null
    - override /logger: null

# we override default configurations with nulls to prevent them from loading at all
# instead we define all modules and their paths directly in this config,
# so everything is stored in one place for more readibility

seed: 12345

task_name: onepose
exp_name: train_onepose
trainer:
    _target_: lightning.pytorch.Trainer
#    devices: -1
    devices: [0]
    min_epochs: 1
    max_epochs: 41
#    max_steps: 250000
    gradient_clip_val: 0.5
    accumulate_grad_batches: 2
    num_sanity_val_steps: 0
    limit_val_batches: 0
#    limit_train_batches: 10
    benchmark: True
    strategy: ddp_find_unused_parameters_true

model:
    # _target_: src.models.spg_model.LitModelSPG
    _target_: src.models.Pose_lightning_model.LitModelPose
    optimizer: adamw
    lr: 5e-4
    weight_decay: 1e-4
    architecture: SuperGlue

    milestones: [10, 20, 30]
    gamma: 0.5

    descriptor_dim: 128
    keypoints_encoder: [32, 64, 128]
    sinkhorn_iterations: 100
    match_threshold: 0.2
    match_type: 'softmax'
    scale_factor: 0.07

    # focal loss
    focal_loss_alpha: 0.5
    focal_loss_gamma: 2
    pos_weights: 0.5
    neg_weights: 0.5

    # GATs
    include_self: True
    with_linear_transform: False
    additional: False

    # SuperPoint
    spp_model_path: ${work_dir}/data/models/extractors/SuperPoint/superpoint_v1.pth

    # ResNet
    resnet_path: ${work_dir}/data/pretrained_resnet/resnet34-333f7ec4.pth

    # NeRF rendering
    N_samples: 128
    feat_dim: 128
    num_source_views: 10
    inv_uniform: True
    white_bkgd: False
    N_rand: 3500

    # NeRF Prerained weight
    nerf_weight: ${work_dir}/data/models/checkpoints/onepose_plus_train/nerf_v3.ckpt
    # trainer:
        # n_val_pairs_to_plot: 5
    # Logging frequency
    log_img_step: 2000
    log_scalar_step: 10

    # Feature aggregation
    use_nerf_mlp: False

    # Backbone
    freeze_backbone: False

    # Transformer Params
    head_dim: 128
    n_head: 1
    dustbin: False
    vis_layer: False
    encoder_layers: 6
    decoder_layers: 6

    # Losses
    pose_supervision: True
    pose_scaling: 1e-2
    pose_epoch: 20
    use_mc_loss: True

    test: False

datamodule:
    _target_: src.datamodules.pose_datamodule.PoseDataModule
    data_dirs: ${data_dir}/sfm_model
    dataset_dirs: ${data_dir}/onepose_datasets
    anno_dirs: outputs_${model.match_type}/anno
    train_anno_file: ${work_dir}/data/cache/${task_name}/train.json
    val_anno_file: ${work_dir}/data/cache/${task_name}/val.json
    test_anno_file: ${work_dir}/data/cache/${task_name}/test.json
    batch_size: 1
    num_workers: 0
    num_leaf: 8
    pin_memory: True
    shape2d: 1000
    shape3d: 2000
    assign_pad_val: 0
    load_in_ram: True
    # NeRF params
    num_source_views: 10
    num_total_views: 30
    resolution: 128
    sample_resolution: 64
    # Voxel sampling params
    voxel_samples: 400
    image_samples: 50
    crd_samples: 50
    sigma: 0.75

callbacks:
    model_checkpoint:
        _target_: lightning.pytorch.callbacks.ModelCheckpoint
        monitor: "val/loss"
        save_top_k: -1
        save_last: True
        mode: "min"
        dirpath: '${data_dir}/models/checkpoints/${exp_name}'
        filename: '{epoch}'
    lr_monitor:
        _target_: lightning.pytorch.callbacks.LearningRateMonitor
        logging_interval: 'step'

logger:
    tensorboard:
        _target_: lightning.pytorch.loggers.TensorBoardLogger
        save_dir: '${data_dir}/logs'
        name: ${exp_name}
        default_hp_metric: False

#    neptune:
#        tags: ["best_model"]
#    csv_logger:
#        save_dir: "."

hydra:
    run:
      dir: ${work_dir}